{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7dc0ede",
   "metadata": {},
   "source": [
    "# Comparing regression analysis with:\n",
    "- a neural network\n",
    "- a gaussian process regressor\n",
    "\n",
    "We want to predict the inherent playing value of a rugby player, based on his skills !\n",
    "The overall value is measured by CSR (Compound Skills Rating).\n",
    "The skills are physical and technical attributes.\n",
    "\n",
    "We aim at comparing two algorithms : a sequential neural network, and a Gaussian Process regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb453a3",
   "metadata": {},
   "source": [
    "### Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2838cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gpytorch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import timeit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f935c925",
   "metadata": {},
   "source": [
    "### NB : works better and faster with a GPU..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef64263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    dtype = torch.FloatTensor\n",
    "\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print('GPU Name:', torch.cuda.get_device_name(0))\n",
    "    print('Total GPU Memory:', round(torch.cuda.get_device_properties(0).total_memory/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbeccd9",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca02cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './data/df_tm.csv'\n",
    "data = pd.read_csv(filename, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c3b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['energy'], inplace=True)  # Remove the 'energy' column as it is not needed for regression analysis\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7572300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca370c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate features and target variable\n",
    "# 'csr' is the target variable we want to predict\n",
    "\n",
    "X = data.drop(columns=['csr'])\n",
    "y = data['csr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1074b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# form training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c5fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data for better performance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad7d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# form datasets and dataloaders for PyTorch training\n",
    "class CSRDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "train_dataset = CSRDataset(X_train_scaled, y_train)\n",
    "test_dataset = CSRDataset(X_test_scaled, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1554ce3d",
   "metadata": {},
   "source": [
    "## Regression by Multi Layer Perceptron network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3923a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a simple multilayer perceptron model\n",
    "class CSRNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128):\n",
    "        super(CSRNeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ### x has shape (batch_size, input_size)\n",
    "        x = self.relu(self.fc1(x))  # First layer with ReLU activation\n",
    "        x = self.relu(self.fc2(x))  # Second layer with ReLU activation\n",
    "        x = self.fc3(x)              # Output layer (no activation)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05bfc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping class to prevent overfitting\n",
    "\n",
    "class EarlyStoppingCallback:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        # how many epochs do we accept with validation loss non decreasing\n",
    "        self.patience = patience\n",
    "        # tolerance for non decrease\n",
    "        self.min_delta = min_delta\n",
    "        # how many epochs without validation loss decrease\n",
    "        self.counter = 0\n",
    "        # minimum validation loss to beat\n",
    "        self.min_validation_loss = float('inf')\n",
    "        self.status = False\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        # is the last validation loss better than the current minimum ?\n",
    "        # status = True means stop training\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            # yes : update minimum value and reset counter\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "            self.status = False\n",
    "            # no : are we within tolerance ?\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            # no : increase counter (losing patience)\n",
    "            self.counter += 1\n",
    "            # have we lost patience ?\n",
    "            if self.counter >= self.patience:\n",
    "                # yes\n",
    "                self.status = True\n",
    "            else:\n",
    "                # no\n",
    "                self.status = False\n",
    "\n",
    "        return self.status, self.counter\n",
    "    \n",
    "early_stopping = EarlyStoppingCallback(patience=5, min_delta=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064206fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for regression\n",
    "\n",
    "model = CSRNeuralNetwork(input_size=X_train_scaled.shape[1])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbef8607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Training Step\n",
    "def train_step(model, train_loader, learning_rate, criterion, optimizer):\n",
    "    \"\"\"Perform one training step on the model.\"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(X_batch)  # Forward pass\n",
    "        loss = criterion(outputs.squeeze(), y_batch)\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return model, epoch_loss/len(train_loader)\n",
    "\n",
    "# One Test Step\n",
    "def test_step(model, test_loader, criterion):\n",
    "    \"\"\"Perform one test step on the model.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)  # Forward pass\n",
    "            loss = criterion(outputs.squeeze(), y_batch)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    return test_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e83930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "def train_model(model, train_loader, test_loader, n_epochs, learning_rate, criterion, optimizer, early_stopping=None):\n",
    "    \"\"\"Train the model with early stopping.\"\"\"\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Train step\n",
    "        model, train_loss = train_step(model, train_loader, learning_rate, criterion, optimizer)\n",
    "        training_losses.append(train_loss)\n",
    "\n",
    "        # Test step\n",
    "        validation_loss = test_step(model, test_loader, criterion)\n",
    "        validation_losses.append(validation_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch+1:<6} .../... {n_epochs:<6}], Training Loss: {train_loss:.4e}, Validation Loss: {validation_loss:.4e}')\n",
    "\n",
    "        # Early stopping check\n",
    "        if early_stopping:\n",
    "            stop, counter = early_stopping.early_stop(validation_loss)\n",
    "            if stop:\n",
    "                print(f'Early stopping at epoch {epoch+1} with counter {counter}')\n",
    "                break\n",
    "\n",
    "    return model, training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a52a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "\n",
    "trained_model, training_losses, validation_losses = train_model(\n",
    "    model, train_loader, test_loader, n_epochs, learning_rate, criterion, optimizer, early_stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d1956",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(training_losses, label='Training Loss', color='blue')\n",
    "ax.plot(validation_losses, label='Validation Loss', color='orange')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss (log scale)')\n",
    "ax.set_yscale('log')  # Use logarithmic scale for better visibility\n",
    "ax.set_title('Training and Validation Losses')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b04d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run metrics on the prediction by the trained model\n",
    "\n",
    "def run_metrics(model, test_loader):\n",
    "    \"\"\"Run metrics on the predictions made by the model.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            predictions.extend(outputs.squeeze().numpy())\n",
    "            targets.extend(y_batch.numpy())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    mse = np.mean((predictions - targets) ** 2)\n",
    "    mae = np.mean(np.abs(predictions - targets))\n",
    "    \n",
    "    print(f'Root Mean Squared Error: {np.sqrt(mse):.0f}')\n",
    "    print(f'Mean Absolute Error: {mae:.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics(trained_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b10b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = trained_model(torch.tensor(X_test_scaled, dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "df_test = pd.DataFrame(\n",
    "    {\n",
    "        'reel' : list(y_test),\n",
    "        'model' : list(y_pred)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec2190",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.sort_values(by=['reel'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26858449",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,6))\n",
    "\n",
    "ax.plot(df_test['reel'].to_numpy(), label='target')\n",
    "ax.plot(df_test['model'].to_numpy(), label='predicted')\n",
    "ax.grid(True)\n",
    "ax.set_ylabel('csr')\n",
    "ax.set_title('MLP : predictions on test set, sorted by target value')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefff664",
   "metadata": {},
   "source": [
    "## Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1727ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is X_train, y_train and X_test, y_test\n",
    "N = 20000  # limited by gpu capacity.\n",
    "\n",
    "X_train_t = torch.tensor(X_train[:N].values, dtype=torch.float32)\n",
    "X_test_t = torch.tensor(X_test[:N].values, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train[:N].values, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test[:N].values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101611e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Process Regressor Model\n",
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        \n",
    "        \"\"\"An __init__ method that takes the training data and a likelihood, \n",
    "        and constructs whatever objects are necessary for the model\"s forward method. \n",
    "        This will most commonly include things like a mean module and a kernel module.\n",
    "        \"\"\"\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        # constant mean as prior mean\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        # RBF kernel as prior covariance\n",
    "        # NB : in GPyTorch, the white noise is handled by the likelihood, not the kernel\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"A forward method that takes in some n x d data x and returns a MultivariateNormal with the prior mean and covariance evaluated at x. \n",
    "        In other words, we return the vector mu(x) and the n x n matrix representing the prior mean and covariance matrix of the GP.\n",
    "        \"\"\"\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f839042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize likelihood\n",
    "# The simplest likelihood for regression is the gpytorch.likelihoods.GaussianLikelihood. \n",
    "# This assumes a homoskedastic noise model (i.e. all inputs have the same observational noise).\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_prior=gpytorch.priors.NormalPrior(1e+4, 1e+5), learn_additional_noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2448464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = ExactGPModel(X_train_t, y_train_t, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e19f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on GPU if available\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Going to GPU\")\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()\n",
    "    X_train_t = X_train_t.cuda()\n",
    "    y_train_t = y_train_t.cuda()\n",
    "    X_test_t = X_test_t.cuda()\n",
    "    y_test_t = y_test_t.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f412a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n epochs\n",
    "training_iter = 0\n",
    "iter_max = 3000  # maximum number of iterations for training\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "# error threshold\n",
    "error_threshold = 1e-8\n",
    "previous_loss = np.inf\n",
    "error = np.inf\n",
    "\n",
    "# training loop\n",
    "t1 = timeit.default_timer()\n",
    "print(\"Training GP model with GP Torch...\")\n",
    "\n",
    "# while error > error_threshold and training_iter <= iter_max:\n",
    "while training_iter <= iter_max:\n",
    "# for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(X_train_t)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # check error\n",
    "    if training_iter >= 1:\n",
    "        error = np.abs(previous_loss - loss.item())/ previous_loss\n",
    "    previous_loss = loss.item()\n",
    "    # report out\n",
    "    if training_iter % 10 == 0:\n",
    "        print(f\"Iteration [{training_iter+1:<6} .../... {iter_max:<6}], Loss: {loss.item():.3e}, Error decrease on last iteration: {error*100:.2e}%, lengthscale : {model.covar_module.base_kernel.lengthscale.item():.2e}, noise (variance): {model.likelihood.noise.item():.2e}\")\n",
    "    # next iteration\n",
    "    training_iter += 1\n",
    "    \n",
    "t2 = timeit.default_timer()\n",
    "print(\"Training time: {:.2f} seconds\".format(t2 - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d1c50",
   "metadata": {},
   "source": [
    "Plot model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195e6705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Number of test points\n",
    "N = 200\n",
    "OFFSET = 200000\n",
    "\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    observed_pred = likelihood(model(X_test_t[:N]))  # returns a MultivariateNormal object, not a tensor !\n",
    "\n",
    "# Plotting the results\n",
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(2, 1, figsize=(20, 6))\n",
    "    \n",
    "    # plot ground truth\n",
    "    ax[0].plot(y_test_t[:N].cpu().numpy(), 'r', label='Ground Truth')\n",
    "    # plot observed values with credible interval\n",
    "    ax[0].plot(observed_pred.mean.cpu().numpy()+OFFSET, 'k', label='Predicted Values')\n",
    "    ax[0].fill_between(\n",
    "        np.arange(N),\n",
    "        observed_pred.mean.cpu().numpy() + OFFSET - 1.96 * observed_pred.stddev.cpu().numpy(),\n",
    "        observed_pred.mean.cpu().numpy() + OFFSET + 1.96 * observed_pred.stddev.cpu().numpy(),\n",
    "        alpha=0.5,\n",
    "        color='blue',\n",
    "        label='95% Confidence Interval'\n",
    "    )\n",
    "    # Set title and labels\n",
    "    ax[0].set_title('GP Regression with GPyTorch')\n",
    "    ax[0].set_xlabel('Test Data Index')\n",
    "    ax[0].set_ylabel('Output Value')\n",
    "\n",
    "    ax[0].legend()\n",
    "    ax[0].grid()\n",
    "    \n",
    "    # lot the residuals\n",
    "    residuals = observed_pred.mean.cpu().numpy() - y_test_t[:N].cpu().numpy()\n",
    "    ax[1].plot(residuals, 'b', label='Residuals')\n",
    "    ax[1].axhline(0, color='red', linestyle='--', label='Zero Line')\n",
    "    ax[1].set_title('Residuals of GP Regression')\n",
    "    ax[1].set_xlabel('Test Data Index')\n",
    "    ax[1].set_ylabel('Residual Value')\n",
    "    \n",
    "    ax[1].fill_between(\n",
    "        np.arange(N),\n",
    "        0 - 1.96 * observed_pred.stddev.cpu().numpy(),\n",
    "        0 + 1.96 * observed_pred.stddev.cpu().numpy(),\n",
    "        alpha=0.5,\n",
    "        color='blue',\n",
    "        label='95% Confidence Interval for Residuals'\n",
    "    )\n",
    "    \n",
    "    ax[1].legend()\n",
    "    ax[1].grid()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metrics\n",
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    full_preds = likelihood(model(X_test_t))\n",
    "\n",
    "print(f'MAE = {mean_absolute_error(y_test_t.cpu(), full_preds.mean.cpu()):.0f}')\n",
    "print(f'RMSE = {np.sqrt(mean_squared_error(y_test_t.cpu(), full_preds.mean.cpu())):.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e50f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(\n",
    "    {\n",
    "        'reel' : list(y_test),\n",
    "        'model' : list(full_preds.mean.cpu().numpy())  # use mean of the predictive distribution\n",
    "    }\n",
    ")\n",
    "df_test.sort_values(by=['reel'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01152342",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,6))\n",
    "\n",
    "ax.plot(df_test['reel'].to_numpy(), label='target')\n",
    "ax.plot(df_test['model'].to_numpy(), label='predicted')\n",
    "ax.grid(True)\n",
    "ax.set_ylabel('csr')\n",
    "ax.set_title('GPR : predictions on test set, sorted by target value')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09395824",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74355fe1",
   "metadata": {},
   "source": [
    "- The simple **Multi Layer Perceptron** is well suited to the regression task, and provides point estimates with good precisions (RMSE, MAE metrics). However, it does not provide credible intervals.\n",
    "- The **Gaussian Process regressor** takes longer to train, as it naively scales in $O(n^3)$, and seems to have a lower precision. However, it can provide credible intervals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
